A linguistic corpus often contains a broad set of linguistic information on a variety of topics, while datasets in other fields are specific to certain instances and can be quantitative rather than linguistic.
A linguistic corpus is compiled texts or speech that serve to answer questions about linguistic phenomena.
A linguistic corpus is generally constituted of a collection of curated linguistic data from a variety of written and (sometimes) spoken language sources for a multitude of purposes; which is contrary to the datasets of other fields in that they are generally constituted of numerical or semantic data for a specific or limited purpose.
I was very interested in the Brown Corpus because although it was initially a very cutting-edge corpus, I thought it was fascinating that the data it contained is not adequate for many research questions in linguistics today, showing the growth of the field. 
The megacorpora stood out to me because of their sheer impressiveness; of these COCA stood out to me due to its claim at being the most "representative" corpus of English.
Switchboard Dialog Act Corpus because it seems to be entirely spoken data of telephone conversations as opposed to being mainly text. This could also allow for more natural use of the language than in text.
I think that the Brown Corpus was initially created with the lofty goal of serving as a definitive collection of texts and linguistic information that could be used for research, and although it has not maintained its position as a definitive source, it is still useful for linguistic research even in the modern era. 
As I implied, COCA was created to be the most "representative" corpus of English and thus purposefully includes specific proportions of different kinds of media to maximize its representativeness of all English that is produced.
Allows for more natural use of the language in a spoken context than simply having people write out their conversations or obtaining data from their written texts. 
Even in the same language, there can be a variety of different dialects and ways of using the language, and when creating a corpus there is a lot of discretion used when choosing sources, meaning that there are a nearly infinite number of different source combinations that corpora can utilize which leads to differences. 
Every linguist thinks that the materials that *they* compile and the ways *they* annotate it are relevant to a linguistic question they want to answer; not all corpora can answer the same question in the same ways. To be specific, different corpora will include different information about the speaker/writer, the (morpho)syntactic properties of the writing/speech, etc. and this information will be used to answer different linguistic questions.
There are many different corpora even in the same language because, over time, there has been many changes to language which are worth studying and comparing. 
There are a number of idiosyncrasies that exist even between dialects of the same language which some may feel is necessary to record in isolation (the meaning of the word "biscuit", for instance, is different between American and British English).
It is difficult to obtain legal samples of spontaneously spoken language, and transcription and annotation of this speech can be difficult and expensive.
If speakers are aware of their speech being recorded, then it becomes less spontaneous and therefore less representative of how people actually talk; getting consent to record spontaneous speech is thus ethically difficult.
Transcribing the spoken conversations for spontaneous corpora can be a hassle or too extensive of a task to do accurately.
Spoken language information is generally more difficult to curate for a corpus, as it includes linguistic information (such as clitics) that may be challenging to tokenize; in addition to this, much of the recorded spoken language available for such a task is merely recited from a written work (newscast, written speeches, etc.).
The Switchboard Corpus contains samples from spontaneous conversations over the phone, which is useful because it shows how speech is used in everyday life rather than in writing or formal presentations such as newscasts.
British Academic Spoken English (BASE)
The sources were intended to be representative of many forms of English in order to provide coverage of as many different styles of text as possible, leading to the inclusion of a variety of different genres.
A mix of sources allows for a broader understanding of the language used, as opposed to concentrating on only one type of composition (such as Science Fiction texts) where the language used is specific to that category. 
I think that this was a good choice because to the native Engish speaker ear, there can often be a level of ambiguity about how a dropped g sounds (at least in my experiences), making it difficult to distinguish between "going" and "goin" at times, so this choice leads to less bias in the way that conversations are transcribed. 
I think this was a fine choice; English speakers at least somewhat conceptualize "we're" as one unit due to its orthographic representation as one word, but we do not think at all about g-drop since, compared to "we're", there is no orthographic alternation.
Do not think it was a good choice because it does not accurately represent the spontaneous speech of American English used by the speakers. 
Though perhaps it was a necessary omission on the part of the developers of the corpus for one reason or another, it seems to me a limitation, as the data does not, in this case, accurately represent the particulars of spoken language.
I believe that I would want to choose a corpus that focuses on spoken data rather than Project Gutenberg because "gonna" is used nearly exclusively in speech and rarely in written texts (with the exception of some dialogue). 
No it would not be; gonna is a mainly spoken phenomenon and Project Gutenberg is a repository of written materials so it would be better to consult a spoken language corpus unless you are specifically researching the transition of gonna into written English.
Project Gutenberg might not be the best resource because it lacks contemporary texts due to copyright reasons and 'gonna' is a term that has been in usage, for the most part, in recent times.
Because most of the information featured on the Gutenberg Project is in the public domain and are largely literary works, it would probably not be a good basis for such analyses, as the language used in pre-1923 literature is unlikely to contain the word "gonna", which has a more contemporary usage.
There seem to be a decent number of code swtiching articles and corpora, but most seem to be limited to American bilingual speakers, and there appears to be a geographic separation of corpora and studies since bilingual speakers in the Southwestern US tend to have a different use of language than speakers in areas like Miami. 
Many of the papers on the subject are not publicly available for viewing; also there is a greater than average concentration of spoken data.
It would be difficult to obtain spoken samples of this code switching because speaker bases for these languages are relatively small and obtaining legal rights to conversations in such a small group would pose a challenge. Additionally, code switching must be spontaneous, so producing the desired result would be hard. 
One challenge is finding enough data to warrant building a corpus since finding bilingual individuals of the two languages may be difficult to amass.
There may be a limited number of linguistic resources that include the simultaneous use of both Flemish and Wallonian; and therefore it may be necessary to either generate one's own resources (such as bilingual interviews) or go to great lengths in procuring linguistic information from what limited resources are available. There may also be some difficulties presented in the actual construction of the corpus, as the annotation and categorization of the linguistic tokens may be a challenging process given any phonetic and/or orthographic similarities between the languages.